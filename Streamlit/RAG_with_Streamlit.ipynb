{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Import Stuff"
      ],
      "metadata": {
        "id": "Y50uaa7cu_T2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7UShXe9u4hK"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch transformers accelerate bitsandbytes transformers sentence-transformers faiss-gpu\n",
        "!pip install -q langchain\n",
        "!pip install -U langchain-community\n",
        "!pip install -q streamlit\n",
        "!pip install pypdf > /dev/null\n",
        "!pip install unstructured > /dev/null\n",
        "!pip install jq > /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG"
      ],
      "metadata": {
        "id": "dvPpVwKYvK76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "from langchain_community.document_loaders import TextLoader, DirectoryLoader, PyPDFLoader, CSVLoader, UnstructuredMarkdownLoader, JSONLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import streamlit as st\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "import os\n",
        "\n",
        "# Cache the document loading and processing\n",
        "@st.cache_data\n",
        "def load_and_process_documents(file_path):\n",
        "    documents = []\n",
        "    for file in os.listdir(file_path):\n",
        "      if file==\"logs.txt\":\n",
        "        continue\n",
        "      if file.endswith('.txt'):\n",
        "        text_file_path = os.path.join(file_path, file)\n",
        "        loader = TextLoader(text_file_path)\n",
        "        documents.extend(loader.load())\n",
        "      elif file.endswith('.pdf'):\n",
        "        pdf_file_path = os.path.join(file_path, file)\n",
        "        loader = PyPDFLoader(pdf_file_path)\n",
        "        documents.extend(loader.load())\n",
        "      elif file.endswith('.csv'):\n",
        "        csv_file_path = os.path.join(file_path, file)\n",
        "        loader = CSVLoader(csv_file_path)\n",
        "        documents.extend(loader.load())\n",
        "      elif file.endswith('.md'):\n",
        "        md_file_path = os.path.join(file_path, file)\n",
        "        loader = UnstructuredMarkdownLoader(md_file_path)\n",
        "        documents.extend(loader.load())\n",
        "    # loader = TextLoader(file_path)\n",
        "    # docs = loader.load()\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=30)\n",
        "    chunked_docs = splitter.split_documents(documents)\n",
        "    return chunked_docs\n",
        "\n",
        "# Cache the model loading\n",
        "@st.cache_resource\n",
        "def load_model_and_tokenizer(model_name, bnb_config):\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "chunked_docs = load_and_process_documents(\"/content/\")\n",
        "\n",
        "embedding_model = \"BAAI/bge-base-en-v1.5\"\n",
        "db = FAISS.from_documents(chunked_docs, HuggingFaceEmbeddings(model_name=embedding_model))\n",
        "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n",
        "\n",
        "model_name = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "model, tokenizer = load_model_and_tokenizer(model_name, bnb_config)\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "Always start your response with \"Hey Himanshu!\"\n",
        "Answer the question based on your knowledge. Use the following context to help:\n",
        "{context}\n",
        "</s>\n",
        "\n",
        "{question}\n",
        "</s>\n",
        "\"\"\"\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=prompt_template,\n",
        ")\n",
        "\n",
        "def run_model(inputs):\n",
        "    context = inputs['context']\n",
        "    question = inputs['question']\n",
        "    formatted_input = prompt.format(context=context, question=question)\n",
        "    input_ids = tokenizer(formatted_input, return_tensors=\"pt\").input_ids\n",
        "    outputs = model.generate(input_ids, max_length=512, max_new_tokens=150)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "def run_rag_chain(question):\n",
        "    context_docs = retriever.get_relevant_documents(question)\n",
        "    context = \" \".join([doc.page_content for doc in context_docs])\n",
        "    response = run_model({\"context\": context, \"question\": question})\n",
        "    return response\n",
        "\n",
        "st.title(\"RAG-based Question Answering\")\n",
        "\n",
        "question = st.text_input(\"Enter your question:\")\n",
        "if question:\n",
        "    response = run_rag_chain(question)\n",
        "    st.write(response)\n"
      ],
      "metadata": {
        "id": "d6fwhwSMvL47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run using localtunnel"
      ],
      "metadata": {
        "id": "4yh5uU4QvN4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel\n",
        "!streamlit run app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ],
      "metadata": {
        "id": "bHJO9GMfvQUE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}